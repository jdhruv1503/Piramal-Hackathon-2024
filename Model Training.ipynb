{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_train.pkl\", \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(\"y_train.pkl\", \"rb\") as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install catboost --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# import optuna\n",
    "\n",
    "# # Step 2: Train-test split\n",
    "# def split_data(X, y, test_size=0.2):\n",
    "#     return train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "\n",
    "# # Step 3: Optuna optimization for XGBoost\n",
    "# def optimize_xgboost(X_train, y_train, X_test, y_test):\n",
    "#     def objective(trial):\n",
    "#         param = {\n",
    "#             'device': 'cuda',\n",
    "#             'objective': 'binary:logistic',\n",
    "#             'eval_metric': 'auc',\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#             'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#             'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "#         }\n",
    "#         model = XGBClassifier(**param)\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Evaluate on the test set\n",
    "#         y_pred = model.predict_proba(X_test)[:, 1]\n",
    "#         score = roc_auc_score(y_test, y_pred)\n",
    "#         return score\n",
    "\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=35)\n",
    "#     print(f\"Best XGBoost params: {study.best_params}, Best ROC AUC: {study.best_value}\")\n",
    "#     return study.best_params\n",
    "\n",
    "# # Step 4: Optuna optimization for LightGBM\n",
    "# def optimize_lgbm(X_train, y_train, X_test, y_test):\n",
    "#     def objective(trial):\n",
    "#         param = {\n",
    "#             'boosting_type': 'gbdt',\n",
    "#             'objective': 'binary',\n",
    "#             'metric': 'auc',\n",
    "#             'device': 'gpu',\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#             'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#             'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "#         }\n",
    "#         model = LGBMClassifier(**param)\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Evaluate on the test set\n",
    "#         y_pred = model.predict_proba(X_test)[:, 1]\n",
    "#         score = roc_auc_score(y_test, y_pred)\n",
    "#         return score\n",
    "\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=35)\n",
    "#     print(f\"Best LightGBM params: {study.best_params}, Best ROC AUC: {study.best_value}\")\n",
    "#     return study.best_params\n",
    "\n",
    "# # Step 5: Optuna optimization for CatBoost\n",
    "# def optimize_catboost(X_train, y_train, X_test, y_test):\n",
    "#     def objective(trial):\n",
    "#         param = {\n",
    "#             'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "#             'depth': trial.suggest_int('depth', 3, 10),\n",
    "#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "#             'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 1e2, log=True),\n",
    "#             'bootstrap_type': 'Bayesian',\n",
    "#             'eval_metric': 'AUC',\n",
    "#             'task_type': 'GPU'\n",
    "#         }\n",
    "#         model = CatBoostClassifier(**param, verbose=0)\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Evaluate on the test set\n",
    "#         y_pred = model.predict_proba(X_test)[:, 1]\n",
    "#         score = roc_auc_score(y_test, y_pred)\n",
    "#         return score\n",
    "\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=35)\n",
    "#     print(f\"Best CatBoost params: {study.best_params}, Best ROC AUC: {study.best_value}\")\n",
    "#     return study.best_params\n",
    "\n",
    "# # Step 6: Execute the full pipeline\n",
    "# def run_pipeline(X, y, target_column='label'):\n",
    "\n",
    "#     # Train-test split\n",
    "#     X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "#     # Optimize and train XGBoost\n",
    "#     print(\"Optimizing XGBoost...\")\n",
    "#     best_xgb_params = optimize_xgboost(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#     # Optimize and train LightGBM\n",
    "#     print(\"Optimizing LightGBM...\")\n",
    "#     best_lgbm_params = optimize_lgbm(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#     # Optimize and train CatBoost\n",
    "#     print(\"Optimizing CatBoost...\")\n",
    "#     best_cat_params = optimize_catboost(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#     return best_xgb_params, best_lgbm_params, best_cat_params\n",
    "\n",
    "\n",
    "# # Run the pipeline\n",
    "# best_xgb_params, best_lgbm_params, best_cat_params = run_pipeline(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_params, best_lgbm_params, best_cat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Best hyperparameters for each model\n",
    "xgb_params = {\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.034793094619124776,\n",
    "    'n_estimators': 446,\n",
    "    'subsample': 0.8239658185711082,\n",
    "    'colsample_bytree': 0.7774255206860643,\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.018173307757395576,\n",
    "    'n_estimators': 744,\n",
    "    'subsample': 0.5717741583210109,\n",
    "    'colsample_bytree': 0.7970924298184494,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 737,\n",
    "    'depth': 9,\n",
    "    'learning_rate': 0.119201934428552,\n",
    "    'l2_leaf_reg': 29.21372245804415,\n",
    "    'task_type': 'GPU'\n",
    "}\n",
    "\n",
    "    \n",
    "# XGB\n",
    "xgb_model = XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X, y)\n",
    "print(\"Fitted XGB\")\n",
    "\n",
    "lgbm_model = LGBMRegressor(**lgbm_params)\n",
    "lgbm_model.fit(X, y)\n",
    "print(\"Fitted LGBM\")\n",
    "\n",
    "# CatBoost\n",
    "catboost_model = CatBoostRegressor(**catboost_params)\n",
    "catboost_model.fit(X, y)\n",
    "print(\"Fitted CatBoost\")\n",
    "\n",
    "# Save the models\n",
    "with open(\"xgb_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "with open(\"lgbm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lgbm_model, f)\n",
    "\n",
    "with open(\"catboost_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(catboost_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# Best hyperparameters for each model\n",
    "xgb_params = {\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.034793094619124776,\n",
    "    'n_estimators': 446,\n",
    "    'subsample': 0.8239658185711082,\n",
    "    'colsample_bytree': 0.7774255206860643,\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.018173307757395576,\n",
    "    'n_estimators': 744,\n",
    "    'subsample': 0.5717741583210109,\n",
    "    'colsample_bytree': 0.7970924298184494,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 737,\n",
    "    'depth': 9,\n",
    "    'learning_rate': 0.119201934428552,\n",
    "    'l2_leaf_reg': 29.21372245804415,\n",
    "    'task_type': 'GPU'\n",
    "}\n",
    "\n",
    "# DataFrames to store predictions\n",
    "oof_preds_xgb = np.zeros(len(X))\n",
    "oof_preds_lgbm = np.zeros(len(X))\n",
    "oof_preds_catboost = np.zeros(len(X))\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(\"-- Fold 1\")\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # XGB\n",
    "    xgb_model = XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    oof_preds_xgb[test_idx] = xgb_model.predict(X_test)\n",
    "    print(\"Fitted XGB\")\n",
    "\n",
    "    # LGBM\n",
    "    lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    oof_preds_lgbm[test_idx] = lgbm_model.predict(X_test)\n",
    "    print(\"Fitted LGBM\")\n",
    "    \n",
    "    # CatBoost\n",
    "    catboost_model = CatBoostClassifier(**catboost_params)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    oof_preds_catboost[test_idx] = catboost_model.predict(X_test)\n",
    "\n",
    "# Create a stacked dataframe with predictions and actual label\n",
    "stacked_df = pd.DataFrame({\n",
    "    'XGB_Preds': oof_preds_xgb,\n",
    "    'LGBM_Preds': oof_preds_lgbm,\n",
    "    'CatBoost_Preds': oof_preds_catboost,\n",
    "    'Label': y\n",
    "})\n",
    "\n",
    "stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_df = pd.read_csv(\"stacked_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_pickle(\"y_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate ROC AUC for each model's predictions\n",
    "roc_auc_xgb = roc_auc_score(y_train, stacked_df['XGB_Preds'])\n",
    "roc_auc_lgbm = roc_auc_score(y_train, stacked_df['LGBM_Preds'])\n",
    "roc_auc_catboost = roc_auc_score(y_train, stacked_df['CatBoost_Preds'])\n",
    "\n",
    "print(f\"ROC AUC for XGB: {roc_auc_xgb}\")\n",
    "print(f\"ROC AUC for LGBM: {roc_auc_lgbm}\")\n",
    "print(f\"ROC AUC for CatBoost: {roc_auc_catboost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Combine the predictions into a single DataFrame\n",
    "X_stacked = stacked_df[['XGB_Preds', 'LGBM_Preds', 'CatBoost_Preds']]\n",
    "y_stacked = stacked_df['Label']\n",
    "\n",
    "# Train a logistic regression model with CalibratedClassifierCV\n",
    "base_model = LogisticRegression()\n",
    "calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid')\n",
    "calibrated_model.fit(X_stacked, y_stacked)\n",
    "\n",
    "# Predict probabilities\n",
    "stacked_probs = calibrated_model.predict_proba(X_stacked)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc_stacked = roc_auc_score(y_stacked, stacked_probs)\n",
    "print(f\"ROC AUC for Stacked Model: {roc_auc_stacked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# Best hyperparameters for each model\n",
    "xgb_params = {\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.034793094619124776,\n",
    "    'n_estimators': 446,\n",
    "    'subsample': 0.8239658185711082,\n",
    "    'colsample_bytree': 0.7774255206860643,\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.018173307757395576,\n",
    "    'n_estimators': 744,\n",
    "    'subsample': 0.5717741583210109,\n",
    "    'colsample_bytree': 0.7970924298184494,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 737,\n",
    "    'depth': 9,\n",
    "    'learning_rate': 0.119201934428552,\n",
    "    'l2_leaf_reg': 29.21372245804415,\n",
    "    'task_type': 'GPU'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"y_train.pkl\")\n",
    "X_test = pd.read_pickle(\"X_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new classifier models with the given hyperparameters\n",
    "new_xgb_model = XGBClassifier(**xgb_params)\n",
    "new_lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "new_catboost_model = CatBoostClassifier(**catboost_params)\n",
    "\n",
    "# Fit the models on the entire training set\n",
    "new_xgb_model.fit(X_train, y_train)\n",
    "new_lgbm_model.fit(X_train, y_train)\n",
    "new_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"New XGB Classifier fitted\")\n",
    "print(\"New LGBM Classifier fitted\")\n",
    "print(\"New CatBoost Classifier fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_xgb[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_xgb = new_xgb_model.predict_proba(X_train)[:, 1]\n",
    "oof_preds_lgbm = new_lgbm_model.predict_proba(X_train)[:, 1]\n",
    "oof_preds_catboost = new_catboost_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "stacked_df = pd.DataFrame({\n",
    "    'XGB_Preds': oof_preds_xgb,\n",
    "    'LGBM_Preds': oof_preds_lgbm,\n",
    "    'CatBoost_Preds': oof_preds_catboost,\n",
    "    'Label': y_train\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC for each model's predictions\n",
    "roc_auc_xgb = roc_auc_score(y_train, stacked_df['XGB_Preds'])\n",
    "roc_auc_lgbm = roc_auc_score(y_train, stacked_df['LGBM_Preds'])\n",
    "roc_auc_catboost = roc_auc_score(y_train, stacked_df['CatBoost_Preds'])\n",
    "\n",
    "print(f\"ROC AUC for XGB: {roc_auc_xgb}\")\n",
    "print(f\"ROC AUC for LGBM: {roc_auc_lgbm}\")\n",
    "print(f\"ROC AUC for CatBoost: {roc_auc_catboost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Combine the predictions into a single DataFrame\n",
    "X_stacked = stacked_df[['XGB_Preds', 'LGBM_Preds', 'CatBoost_Preds']]\n",
    "y_stacked = stacked_df['Label']\n",
    "\n",
    "# Train a logistic regression model with CalibratedClassifierCV\n",
    "base_model = LogisticRegression()\n",
    "calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid')\n",
    "calibrated_model.fit(X_stacked, y_stacked)\n",
    "\n",
    "# Predict probabilities\n",
    "stacked_probs = calibrated_model.predict_proba(X_stacked)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc_stacked = roc_auc_score(y_stacked, stacked_probs)\n",
    "print(f\"ROC AUC for Stacked Model: {roc_auc_stacked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "oof_preds_xgb = new_xgb_model.predict_proba(X_test)[:, 1]\n",
    "oof_preds_lgbm = new_lgbm_model.predict_proba(X_test)[:, 1]\n",
    "oof_preds_catboost = new_catboost_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "stacked_df = pd.DataFrame({\n",
    "    'XGB_Preds': oof_preds_xgb,\n",
    "    'LGBM_Preds': oof_preds_lgbm,\n",
    "    'CatBoost_Preds': oof_preds_catboost\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "stacked_probs = calibrated_model.predict_proba(stacked_df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test1 = pd.read_csv('test_1.csv')\n",
    "submission = pd.DataFrame({'loan_id': test1['loan_id'], 'prob': stacked_probs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "for i,_ in enumerate([oof_preds_xgb, oof_preds_lgbm, oof_preds_catboost]):\n",
    "    submission = pd.DataFrame({'loan_id': test1['loan_id'], 'prob': _})\n",
    "    submission.to_csv(f'submission_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the LightGBM model with the best parameters\n",
    "lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "\n",
    "# Initialize the CalibratedClassifierCV\n",
    "calibrated_lgbm = CalibratedClassifierCV(lgbm_model, method='sigmoid', cv=kf)\n",
    "\n",
    "# Fit the calibrated model on the entire training set\n",
    "calibrated_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "stacked_probs_lgbm = calibrated_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission_lgbm = pd.DataFrame({'loan_id': test1['loan_id'], 'prob': stacked_probs_lgbm})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_lgbm.to_csv('submission_lgbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
