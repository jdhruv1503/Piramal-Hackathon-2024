{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('train_1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str_obj_cols = train_1.columns[train_1.dtypes == \"object\"].tolist()\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    train_1[str_obj_col] = train_1[str_obj_col].astype(\"category\")\n",
    "\n",
    "list_str_obj_cols = ['loan_id', 'id']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    train_1[str_obj_col] = train_1[str_obj_col].astype(\"object\")\n",
    "\n",
    "list_str_obj_cols = ['label']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    train_1[str_obj_col] = train_1[str_obj_col].astype(\"bool\")\n",
    "\n",
    "train_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.to_pickle('train_1_proc.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1 = pd.read_csv('train_2_1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1[train_2_1.columns[train_2_1.dtypes == \"object\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str_obj_cols = ['add_431']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    if str_obj_col in train_2_1.columns:\n",
    "        train_2_1[str_obj_col] = pd.to_datetime(train_2_1[str_obj_col], errors='coerce')\n",
    "        \n",
    "        # Add year, month, day, weekday, and weekend columns\n",
    "        train_2_1[f'{str_obj_col}_year'] = train_2_1[str_obj_col].dt.year\n",
    "        train_2_1[f'{str_obj_col}_month'] = train_2_1[str_obj_col].dt.month\n",
    "        train_2_1[f'{str_obj_col}_day'] = train_2_1[str_obj_col].dt.day\n",
    "        train_2_1[f'{str_obj_col}_weekday'] = train_2_1[str_obj_col].dt.weekday\n",
    "        train_2_1[f'{str_obj_col}_is_weekend'] = train_2_1[str_obj_col].dt.weekday >= 5\n",
    "\n",
    "        # Drop the original date column\n",
    "        train_2_1.drop(columns=[str_obj_col], inplace=True)\n",
    "\n",
    "list_str_obj_cols = ['add_671', 'add_672', 'add_673', 'add_675', 'add_676', 'add_677']\n",
    "\n",
    "def safe_eval_array(x):\n",
    "    try:\n",
    "        x = re.sub(r'\\b0+(\\d)', r'\\1', x)\n",
    "        x = ast.literal_eval(x)\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def unwrap_arrays(data, columns):\n",
    "    for col in columns:\n",
    "        # Step 1: Convert string to lists using safe_eval_array\n",
    "        mySeries = data[col].values\n",
    "        mySeries = mySeries.tolist()\n",
    "        mySeries = [safe_eval_array(x) for x in mySeries]\n",
    "        mySeries = pd.Series(mySeries)\n",
    "        # print(col)\n",
    "        # print(mySeries)\n",
    "\n",
    "        # Step 3: Find the maximum length of the arrays in the column\n",
    "        max_len = mySeries.apply(len).max()\n",
    "\n",
    "        # Step 4: Create separate columns for each element in the arrays\n",
    "        exploded_cols = pd.DataFrame(\n",
    "            mySeries.apply(lambda x: x + [np.nan] * (max_len - len(x))).tolist(),\n",
    "            index=data.index,\n",
    "            columns=[f'{col}_elem_{i+1}' for i in range(max_len)]\n",
    "        )\n",
    "\n",
    "        # Step 5: Calculate the mean and length of the arrays\n",
    "        data[f'{col}_mean'] = mySeries.apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "        data[f'{col}_len'] = mySeries.apply(len)\n",
    "\n",
    "        # Step 6: Concatenate the exploded columns to the original dataframe\n",
    "        data = pd.concat([data, exploded_cols], axis=1)\n",
    "\n",
    "        # Step 7: Drop the original column\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_2_1 = unwrap_arrays(train_2_1, list_str_obj_cols)\n",
    "\n",
    "train_2_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1.to_pickle('train_2_1_proc.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1 = pd.read_csv('train_2_2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1[train_2_1.columns[train_2_1.dtypes == \"object\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str_obj_cols = ['add_431']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    if str_obj_col in train_2_1.columns:\n",
    "        train_2_1[str_obj_col] = pd.to_datetime(train_2_1[str_obj_col], errors='coerce')\n",
    "        \n",
    "        # Add year, month, day, weekday, and weekend columns\n",
    "        train_2_1[f'{str_obj_col}_year'] = train_2_1[str_obj_col].dt.year\n",
    "        train_2_1[f'{str_obj_col}_month'] = train_2_1[str_obj_col].dt.month\n",
    "        train_2_1[f'{str_obj_col}_day'] = train_2_1[str_obj_col].dt.day\n",
    "        train_2_1[f'{str_obj_col}_weekday'] = train_2_1[str_obj_col].dt.weekday\n",
    "        train_2_1[f'{str_obj_col}_is_weekend'] = train_2_1[str_obj_col].dt.weekday >= 5\n",
    "\n",
    "        # Drop the original date column\n",
    "        train_2_1.drop(columns=[str_obj_col], inplace=True)\n",
    "\n",
    "list_str_obj_cols = ['add_671', 'add_672', 'add_673', 'add_675', 'add_676', 'add_677']\n",
    "\n",
    "train_2_1 = unwrap_arrays(train_2_1, list_str_obj_cols)\n",
    "\n",
    "train_2_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1.to_pickle('train_2_2_proc.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
