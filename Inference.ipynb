{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('test_1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str_obj_cols = train_1.columns[train_1.dtypes == \"object\"].tolist()\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    train_1[str_obj_col] = train_1[str_obj_col].astype(\"category\")\n",
    "\n",
    "list_str_obj_cols = ['loan_id', 'id']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    train_1[str_obj_col] = train_1[str_obj_col].astype(\"object\")\n",
    "\n",
    "train_1.dtypes\n",
    "train_1.to_pickle('test_1_proc.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1 = pd.read_csv('test_2_1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str_obj_cols = ['add_431']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    if str_obj_col in train_2_1.columns:\n",
    "        train_2_1[str_obj_col] = pd.to_datetime(train_2_1[str_obj_col], errors='coerce')\n",
    "        \n",
    "        # Add year, month, day, weekday, and weekend columns\n",
    "        train_2_1[f'{str_obj_col}_year'] = train_2_1[str_obj_col].dt.year\n",
    "        train_2_1[f'{str_obj_col}_month'] = train_2_1[str_obj_col].dt.month\n",
    "        train_2_1[f'{str_obj_col}_day'] = train_2_1[str_obj_col].dt.day\n",
    "        train_2_1[f'{str_obj_col}_weekday'] = train_2_1[str_obj_col].dt.weekday\n",
    "        train_2_1[f'{str_obj_col}_is_weekend'] = train_2_1[str_obj_col].dt.weekday >= 5\n",
    "\n",
    "        # Drop the original date column\n",
    "        train_2_1.drop(columns=[str_obj_col], inplace=True)\n",
    "\n",
    "list_str_obj_cols = ['add_671', 'add_672', 'add_673', 'add_675', 'add_676', 'add_677']\n",
    "\n",
    "def safe_eval_array(x):\n",
    "    try:\n",
    "        x = re.sub(r'\\b0+(\\d)', r'\\1', x)\n",
    "        x = ast.literal_eval(x)\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def unwrap_arrays(data, columns):\n",
    "    for col in columns:\n",
    "        # Step 1: Convert string to lists using safe_eval_array\n",
    "        mySeries = data[col].values\n",
    "        mySeries = mySeries.tolist()\n",
    "        mySeries = [safe_eval_array(x) for x in mySeries]\n",
    "        mySeries = pd.Series(mySeries)\n",
    "        # print(col)\n",
    "        # print(mySeries)\n",
    "\n",
    "        # Step 3: Find the maximum length of the arrays in the column\n",
    "        max_len = mySeries.apply(len).max()\n",
    "\n",
    "        # Step 4: Create separate columns for each element in the arrays\n",
    "        exploded_cols = pd.DataFrame(\n",
    "            mySeries.apply(lambda x: x + [np.nan] * (max_len - len(x))).tolist(),\n",
    "            index=data.index,\n",
    "            columns=[f'{col}_elem_{i+1}' for i in range(max_len)]\n",
    "        )\n",
    "\n",
    "        # Step 5: Calculate the mean and length of the arrays\n",
    "        data[f'{col}_mean'] = mySeries.apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "        data[f'{col}_len'] = mySeries.apply(len)\n",
    "\n",
    "        # Step 6: Concatenate the exploded columns to the original dataframe\n",
    "        data = pd.concat([data, exploded_cols], axis=1)\n",
    "\n",
    "        # Step 7: Drop the original column\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_2_1 = unwrap_arrays(train_2_1, list_str_obj_cols)\n",
    "\n",
    "train_2_1.to_pickle('test_2_1_proc.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_1 = pd.read_csv('test_2_2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str_obj_cols = ['add_431']\n",
    "for str_obj_col in list_str_obj_cols:\n",
    "    if str_obj_col in train_2_1.columns:\n",
    "        train_2_1[str_obj_col] = pd.to_datetime(train_2_1[str_obj_col], errors='coerce')\n",
    "        \n",
    "        # Add year, month, day, weekday, and weekend columns\n",
    "        train_2_1[f'{str_obj_col}_year'] = train_2_1[str_obj_col].dt.year\n",
    "        train_2_1[f'{str_obj_col}_month'] = train_2_1[str_obj_col].dt.month\n",
    "        train_2_1[f'{str_obj_col}_day'] = train_2_1[str_obj_col].dt.day\n",
    "        train_2_1[f'{str_obj_col}_weekday'] = train_2_1[str_obj_col].dt.weekday\n",
    "        train_2_1[f'{str_obj_col}_is_weekend'] = train_2_1[str_obj_col].dt.weekday >= 5\n",
    "\n",
    "        # Drop the original date column\n",
    "        train_2_1.drop(columns=[str_obj_col], inplace=True)\n",
    "\n",
    "list_str_obj_cols = ['add_671', 'add_672', 'add_673', 'add_675', 'add_676', 'add_677']\n",
    "\n",
    "def safe_eval_array(x):\n",
    "    try:\n",
    "        x = re.sub(r'\\b0+(\\d)', r'\\1', x)\n",
    "        x = ast.literal_eval(x)\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def unwrap_arrays(data, columns):\n",
    "    for col in columns:\n",
    "        # Step 1: Convert string to lists using safe_eval_array\n",
    "        mySeries = data[col].values\n",
    "        mySeries = mySeries.tolist()\n",
    "        mySeries = [safe_eval_array(x) for x in mySeries]\n",
    "        mySeries = pd.Series(mySeries)\n",
    "        # print(col)\n",
    "        # print(mySeries)\n",
    "\n",
    "        # Step 3: Find the maximum length of the arrays in the column\n",
    "        max_len = mySeries.apply(len).max()\n",
    "\n",
    "        # Step 4: Create separate columns for each element in the arrays\n",
    "        exploded_cols = pd.DataFrame(\n",
    "            mySeries.apply(lambda x: x + [np.nan] * (max_len - len(x))).tolist(),\n",
    "            index=data.index,\n",
    "            columns=[f'{col}_elem_{i+1}' for i in range(max_len)]\n",
    "        )\n",
    "\n",
    "        # Step 5: Calculate the mean and length of the arrays\n",
    "        data[f'{col}_mean'] = mySeries.apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "        data[f'{col}_len'] = mySeries.apply(len)\n",
    "\n",
    "        # Step 6: Concatenate the exploded columns to the original dataframe\n",
    "        data = pd.concat([data, exploded_cols], axis=1)\n",
    "\n",
    "        # Step 7: Drop the original column\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_2_1 = unwrap_arrays(train_2_1, list_str_obj_cols)\n",
    "\n",
    "train_2_1.to_pickle('test_2_2_proc.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_pickle('test_1_proc.bin')\n",
    "train_2_1 = pd.read_pickle('test_2_1_proc.bin')\n",
    "train_2_2 = pd.read_pickle('test_2_2_proc.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # Needed to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pickle\n",
    "\n",
    "def impute_numeric_categorical(data, imputers):# -> Any:\n",
    "    # Step 1: Separate numerical, categorical, and object columns\n",
    "    num_cols = data.select_dtypes(include=['number']).columns\n",
    "    cat_cols = data.select_dtypes(include=['category']).columns\n",
    "\n",
    "    imputer_num = imputers['numerical']\n",
    "    encoder = imputers['encoder']\n",
    "    imputer_cat = imputers['categorical']\n",
    "\n",
    "    # Step 2: Impute numerical columns with IterativeImputer\n",
    "    if len(num_cols) > 0:\n",
    "        num_data = data[num_cols]\n",
    "        imputed_num_data = imputer_num.transform(num_data[imputer_num.feature_names_in_])\n",
    "        data[imputer_num.feature_names_in_] = imputed_num_data\n",
    "\n",
    "    # Step 3: Impute categorical columns with SimpleImputer (most frequent strategy)\n",
    "    if len(cat_cols) > 0:\n",
    "        # Encode categorical columns temporarily using OrdinalEncoder\n",
    "        encoded_cat_data = encoder.transform(data[encoder.feature_names_in_])\n",
    "\n",
    "        # Impute encoded categorical columns with SimpleImputer\n",
    "        imputed_cat_data = imputer_cat.transform(encoded_cat_data)\n",
    "\n",
    "        # Convert back to original categories using the encoder\n",
    "        imputed_cat_data = encoder.inverse_transform(imputed_cat_data)\n",
    "        data[encoder.feature_names_in_] = imputed_cat_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "with open(\"imputers.pkl\", \"rb\") as f:\n",
    "    imputers = pickle.load(f)\n",
    "\n",
    "imputed_data = impute_numeric_categorical(train_1, imputers['train_1'])\n",
    "\n",
    "imputed_data.to_pickle('test_1_imputed.bin')\n",
    "\n",
    "imputed_data = impute_numeric_categorical(train_2_1, imputers['train_2_1'])\n",
    "\n",
    "imputed_data.to_pickle('test_2_1_imputed.bin')\n",
    "\n",
    "imputed_data = impute_numeric_categorical(train_2_2, imputers['train_2_2'])\n",
    "\n",
    "imputed_data.to_pickle('test_2_2_imputed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_1 = pd.read_pickle('test_1_imputed.bin')\n",
    "train_2_1 = pd.read_pickle('test_2_1_imputed.bin')\n",
    "train_2_2 = pd.read_pickle('test_2_2_imputed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Aggregate train_2_1 at 'id' level\n",
    "adict = {col: ['mean', 'sum', 'max', 'min'] for col in train_2_1.columns if col not in ['id', 'add_431']}\n",
    "agg_train_2_1 = train_2_1.groupby('id').agg(\n",
    "    adict\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names after aggregation\n",
    "agg_train_2_1.columns = ['_'.join(col).strip('_') for col in agg_train_2_1.columns]\n",
    "\n",
    "adict = {col: ['mean', 'sum', 'max', 'min'] for col in train_2_2.columns if col not in ['id', 'add_431']}\n",
    "agg_train_2_2 = train_2_2.groupby('id').agg(\n",
    "    adict\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names after aggregation\n",
    "agg_train_2_2.columns = ['t2_'+'_'.join(col).strip('_') for col in agg_train_2_2.columns]\n",
    "agg_train_2_2.rename(columns={'t2_id': 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train_1.merge(agg_train_2_1, on='id', how='left')\n",
    "train_1 = train_1.merge(agg_train_2_2, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agg_train_2_1\n",
    "del agg_train_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import impute\n",
    "from sklearn.experimental import enable_iterative_imputer  # Needed to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pickle\n",
    "\n",
    "def impute_numeric_categorical(data, imputers2):# -> Any:\n",
    "    # Step 1: Separate numerical, categorical, and object columns\n",
    "    num_cols = data.select_dtypes(include=['number']).columns\n",
    "    cat_cols = data.select_dtypes(include=['category']).columns\n",
    "\n",
    "    imputer_num = imputers2['numerical']\n",
    "    encoder = imputers2['encoder']\n",
    "    imputer_cat = imputers2['categorical']\n",
    "\n",
    "    # Step 2: Impute numerical columns with IterativeImputer\n",
    "    if len(num_cols) > 0:\n",
    "        num_data = data[num_cols]\n",
    "        imputed_num_data = imputer_num.transform(num_data[imputer_num.feature_names_in_])\n",
    "        data[imputer_num.feature_names_in_] = imputed_num_data\n",
    "\n",
    "    # Step 3: Impute categorical columns with SimpleImputer (most frequent strategy)\n",
    "    # if len(cat_cols) > 0:\n",
    "    #     # Encode categorical columns temporarily using OrdinalEncoder\n",
    "    #     encoded_cat_data = encoder.transform(data[encoder.feature_names_in_])\n",
    "\n",
    "    #     # Impute encoded categorical columns with SimpleImputer\n",
    "    #     imputed_cat_data = imputer_cat.transform(encoded_cat_data)\n",
    "\n",
    "    #     # Convert back to original categories using the encoder\n",
    "    #     imputed_cat_data = encoder.inverse_transform(imputed_cat_data)\n",
    "    #     data[encoder.feature_names_in_] = imputed_cat_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "with open('imputers2.pkl', 'rb') as f:\n",
    "    imputers2 = pickle.load(f)\n",
    "\n",
    "# Apply imputation to numerical and categorical columns only\n",
    "imputed_data = impute_numeric_categorical(train_1, imputers2)\n",
    "\n",
    "imputed_data.to_pickle('test_1_agg_imputed.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = pd.read_pickle('test_1_agg_imputed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = pd.merge(imputed_data, oot, left_on=\"loan_id\" ,right_on=\"loan_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = imputed_data.drop(columns=['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = imputed_data.drop(columns=['id', 'loan_id'])\n",
    "labels = imputed_data['loan_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot = pd.read_csv(\"test_oot_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot = oot[oot['Tag'] == 'private']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def preprocess_data(df, scaler, ohe):\n",
    "    # Step 1: Detect categorical and numerical columns\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    # Step 2: One-Hot Encode categorical columns\n",
    "    if len(cat_cols) > 0:\n",
    "        encoded_cat_data = ohe.transform(df[ohe.feature_names_in_])\n",
    "        encoded_cat_df = pd.DataFrame(encoded_cat_data, columns=ohe.get_feature_names_out(ohe.feature_names_in_), index=df.index)\n",
    "    else:\n",
    "        encoded_cat_df = pd.DataFrame(index=df.index)  # Empty DataFrame if no categorical columns\n",
    "\n",
    "    # Step 3: Standard Scale numerical columns\n",
    "    if len(num_cols) > 0:\n",
    "        scaled_num_data = scaler.transform(df[scaler.feature_names_in_])\n",
    "        scaled_num_df = pd.DataFrame(scaled_num_data, columns=scaler.feature_names_in_, index=df.index)\n",
    "    else:\n",
    "        scaled_num_df = pd.DataFrame(index=df.index)  # Empty DataFrame if no numerical columns\n",
    "\n",
    "    # Step 4: Concatenate the processed categorical and numerical data\n",
    "    X = pd.concat([scaled_num_df, encoded_cat_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open('ohe.pkl', 'rb') as f:\n",
    "    ohe = pickle.load(f)\n",
    "\n",
    "# Preprocess the data and split into X and y\n",
    "X = preprocess_data(X, scaler, ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open(\"labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xgb_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_model.pkl\", \"rb\") as f:\n",
    "    model2 = pickle.load(f)\n",
    "\n",
    "with open(\"catboost_model.pkl\", \"rb\") as f:\n",
    "    model3 = pickle.load(f)\n",
    "\n",
    "with open(\"final_calibrated_model.pkl\", \"rb\") as f:\n",
    "    metamodel = pickle.load(f)\n",
    "\n",
    "with open(\"X_test.pkl\", \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Step 1: Predict the probabilities\n",
    "y_pred = model.predict(X)\n",
    "y_pred2 = model2.predict(X)\n",
    "y_pred3 = model3.predict(X)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    'XGB_Preds': y_pred,\n",
    "    'LGBM_Preds': y_pred2,\n",
    "    'CatBoost_Preds': y_pred3\n",
    "})\n",
    "\n",
    "# Step 2: Meta-model prediction\n",
    "final_preds = metamodel.predict_proba(pred_df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
